import streamlit as st
import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

st.title("Cluster Prediction")

#--- ORIGINAL DATA / SYSTEM DATA TO EXTRACT COLUMN TYPES ------

file_path = os.path.join(os.path.dirname(__file__), "../data/DM2425_ABCDEats_DATASET.csv")
df = pd.read_csv(file_path, sep=',')

file_path_clusters = os.path.join(os.path.dirname(__file__), "../data/DM2425_ABCDEats_DATASET_w_Clusters.csv")
df_w_clusters = pd.read_csv(file_path_clusters, sep=',', index_col='customer_id')

# Set customer_id as the index 
df = df.drop_duplicates(subset='customer_id').set_index('customer_id')

# ---- FUNCTIONS ----

def extract_column_info(df):
    """
    Extracts unique values and formats for each column in the dataset.
    """
    column_info = {}
    for col in df.columns:
        if df[col].dtype == 'object':
            column_info[col] = df[col].dropna().unique().tolist()
        elif np.issubdtype(df[col].dtype, np.number):
            column_info[col] = (df[col].min(), df[col].max())
        elif np.issubdtype(df[col].dtype, np.datetime64):
            column_info[col] = "Date format (YYYY-MM-DD)"
    return column_info


# Author : https://www.kaggle.com/gemartin/load-data-reduce-memory-usage and adapted to be able to check if the column is actually a float
#or is an error like a code being a float column - if the column has Nan it will be still a float and will give us a warning.

def reduce_memory_usage(df):
    """Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.
       Uses 32-bit types as the minimum size for optimization."""
    df = df.copy()
    start_mem = df.memory_usage().sum() / 1024**2
    print(f"Memory usage of dataframe is {start_mem:.2f} MB")
    
    for col in df.columns:
        col_type = df[col].dtype

        # Skip categorical columns
        if pd.api.types.is_categorical_dtype(col_type):
            continue

        if np.issubdtype(col_type, np.integer):
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)
            else:
                df[col] = df[col].astype(np.int64)
        
        elif np.issubdtype(col_type, np.floating):
            c_min = df[col].min()
            c_max = df[col].max()
            # Check if the column contains NaN values
            if not df[col].isna().any():
                # Check if all values are integers
                if (df[col] % 1 == 0).all():
                    # Convert to nullable integer type if compatible
                    if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    else:
                        df[col] = df[col].astype(np.int64)
                else:
                    # Optimize as float32 if range allows
                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
                        print(f"Column '{col}' converted to float32")
                    else:
                        print(f"Column '{col}' remains as float64 due to range")
            else:
                # Optimize as float32 if range allows
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                    print(f"Column '{col}' with NaNs converted to float32")
                else:
                    print(f"Column '{col}' with NaNs remains as float64 due to range")
        
        elif col_type == 'object':
            # Convert to category if unique values are below a threshold
            if df[col].nunique() / len(df[col]) < 0.5:
                df[col] = df[col].astype('category')
    
    end_mem = df.memory_usage().sum() / 1024**2
    print(f"Memory usage after optimization is: {end_mem:.2f} MB")
    print(f"Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%")
    
    return df

def encoder_and_scaler(scaler, df, numerical_columns):
    """
    Preprocess data by scaling numerical columns using the specified scaler.

    Args:
        scaler (str): "standard", "minmax", or "robust" - specifies the scaler to use.
        numerical_columns (list): List of numerical columns for scaling.
    """
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

    # Define available scalers
    scalers = {
        "standard": StandardScaler(),
        "minmax": MinMaxScaler(),
        "robust": RobustScaler()}

    if scaler not in scalers:
        raise ValueError("Invalid scaler specified. Choose from 'standard', 'minmax', or 'robust'.")

    scaler_instance = scalers[scaler]

    # Process numerical columns
    scaled_numerical = None
    if numerical_columns:
        scaled_numerical = scaler_instance.fit_transform(df[numerical_columns])

    # Combine transformed data into a DataFrame
    transformed_dfs = []
    if scaled_numerical is not None:
        transformed_dfs.append(pd.DataFrame(scaled_numerical, columns=numerical_columns, index=df.index))

    # Include any columns not transformed
    other_columns = df.drop(columns=numerical_columns, errors='ignore')
    if not other_columns.empty:
        transformed_dfs.append(other_columns)

    # Concatenate all parts into a single DataFrame
    transformed_df = pd.concat(transformed_dfs, axis=1)

    return transformed_df


# ---- PREPARATION PIP ----

rename_dict = {
    'DOW_0': 'Sun_Orders',
    'DOW_1': 'Mon_Orders',
    'DOW_2': 'Tue_Orders',
    'DOW_3': 'Wed_Orders',
    'DOW_4': 'Thu_Orders',
    'DOW_5': 'Fri_Orders',
    'DOW_6': 'Sat_Orders'
}

Asian_Cuisine = [
    'CUI_Asian_spending_dist', 
    'CUI_Street Food / Snacks_spending_dist',
    'CUI_Japanese_spending_dist', 
    'CUI_Desserts_spending_dist',
    'CUI_Beverages_spending_dist',
    'CUI_Healthy_spending_dist'
]

Chinese_Cuisine = [
    'CUI_Chinese_spending_dist', 
    'CUI_Chicken Dishes_spending_dist', 
    'CUI_Noodle Dishes_spending_dist'
]

Western_Cuisine = [
    'CUI_Italian_spending_dist', 
    'CUI_Cafe_spending_dist', 
    'CUI_American_spending_dist'
]

Other_Cuisine = [
    'CUI_OTHER_spending_dist', 
    'CUI_Thai_spending_dist', 
    'CUI_Indian_spending_dist'
]

spending_behavior = ['Asian_Cuisine', 'Chinese_Cuisine', 'Western_Cuisine', 'Other_Cuisine', 'cuisine_diversity']
timing_behavior = ['Weekdays', 'Sat_Orders', 'Sun_Orders', 'breakfast_orders', 'lunch_orders',
                   'afternoon_snack_orders', 'dinner_orders', 'late_night_orders', 'time_of_day_diversity']
customer_behavior = ['antiguity', 'R_recency_days', 'F_total_orders',
                     'avg_products_p_order', 'avg_spend_p_order', 'M_total_spend']

all_perspectives = spending_behavior + timing_behavior + customer_behavior

def prepare_data(df):
    # Step 1: Drop duplicates
    # df.drop_duplicates(inplace=True)

    # Step 2: Fill missing values for `first_order`
    df['first_order'] = df['first_order'].fillna(0)

    # Step 3: Handle `HR_0` null values
    dow_columns = df.filter(regex=r'^DOW_\d').columns.tolist()
    hr_columns_excl_hr0 = ['HR_' + str(i) for i in range(1, 24)]
    df.loc[df['HR_0'].isnull(), 'HR_0'] = (
        df[dow_columns].sum(axis=1) - df[hr_columns_excl_hr0].sum(axis=1)
    )

    # Step 4: Replace "-" in `last_promo` and `customer_region`
    df["last_promo"] = df["last_promo"].replace("-", "No Promo")
    most_frequent_region = df.loc[df["customer_region"] != "-", "customer_region"].mode()[0]
    df["customer_region"] = df["customer_region"].replace("-", most_frequent_region)

    # Step 5: KNN Imputation for `customer_age`
    scaler = StandardScaler()
    df['customer_age_scaled'] = scaler.fit_transform(df[['customer_age']])
    knn_imputer = KNNImputer(n_neighbors=5)
    df['customer_age_scaled'] = knn_imputer.fit_transform(df[['customer_age_scaled']])
    df['customer_age'] = scaler.inverse_transform(df[['customer_age_scaled']])
    df.drop(columns=['customer_age_scaled'], inplace=True)

    # Step 6: Filter rows with specific conditions and provide warnings
    invalid_rows = []

    for idx, row in df.iterrows():
        if row['product_count'] == 0:
            invalid_rows.append((idx, "Your record has a 'product_count' of 0, please confirm this line."))
        elif row['vendor_count'] == 0:
            invalid_rows.append((idx, "Your record has a 'vendor_count' of 0, please confirm this line."))
        elif row.filter(regex=r'^CUI_').sum() == 0:
            invalid_rows.append((idx, "Your record has a total cuisine spending of 0, please confirm this line."))
        elif row.filter(regex=r'^HR_\d').sum() == 0:
            invalid_rows.append((idx, "Your record has no orders in any hour, please confirm this line."))
        elif row[dow_columns].sum() == 0:
            invalid_rows.append((idx, "Your record has no orders on any day, please confirm this line."))
        elif row[dow_columns].sum() != row.filter(regex=r'^HR_\d').sum():
            invalid_rows.append((idx, "Your record's total orders in all hours and all days do not match, please confirm this line."))

    # If there are invalid rows, stop the process and show warnings
    if invalid_rows:
        warning_message = "\n".join([f"Row {idx}: {message}" for idx, message in invalid_rows])
        raise ValueError(f"Invalid records detected:\n{warning_message}")

    # Step 7: Apply manual filtering
    def apply_manual_filtering(df):
        outliers_manual = (
            (df['CUI_OTHER'] <= 122) &
            (df['CUI_Italian'] <= 168) &
            (df['CUI_Japanese'] <= 130) &
            (df['CUI_Beverages'] <= 139) &
            (df["CUI_Street Food / Snacks"] <= 203) &
            (df['CUI_Chinese'] <= 100) &
            (df['CUI_American'] <= 121) &
            (df['CUI_Asian'] <= 252) &
            (df['CUI_Indian'] <= 109) &
            (df['CUI_Chicken Dishes'] <= 50) &
            (df["CUI_Thai"] <= 64) &
            (df['CUI_Noodle Dishes'] <= 70) &
            (df['CUI_Desserts'] <= 74) &
            (df['CUI_Healthy'] <= 80) &
            (df['CUI_Cafe'] <= 90) &
            (df["DOW_0"] <= 11) &
            (df["DOW_1"] <= 13) &
            (df["DOW_3"] <= 14) &
            (df["DOW_5"] <= 15) &
            (df["DOW_6"] <= 14) &
            (df["HR_0"] <= 9) &
            (df["HR_1"] <= 9) &
            (df["HR_2"] <= 10) &
            (df["HR_4"] <= 10) &
            (df["HR_5"] <= 5) &
            (df["HR_7"] <= 9) &
            (df["HR_8"] <= 10) &
            (df["HR_9"] <= 11) &
            (df["HR_10"] <= 17) &
            (df["HR_11"] <= 14) &
            (df["HR_12"] <= 14) &
            (df["HR_13"] <= 12) &
            (df["HR_14"] <= 11) &
            (df["HR_15"] <= 10) &
            (df["HR_16"] <= 13) &
            (df["HR_17"] <= 15) &
            (df["HR_18"] <= 15) &
            (df["HR_19"] <= 13) &
            (df["HR_20"] <= 10) &
            (df["HR_21"] <= 6) &
            (df["HR_22"] <= 7) &
            (df["HR_23"] <= 6) &
            (df['vendor_count'] <= 32) &
            (df['product_count'] <= 97) &
            (df['is_chain'] <= 40)
        )
        outliers = df[~outliers_manual]
        return df[outliers_manual], outliers

    df, outliers = apply_manual_filtering(df)

    # Step 8: Add derived features
    df.rename(columns=rename_dict, inplace=True)
    cuisine_columns = [col for col in df.columns if col.startswith('CUI_')]
    dow_columns = [col for col in df.columns if col.endswith('_Orders')]
    hour_columns = [col for col in df.columns if col.startswith('HR_')]

    df['antiguity'] = df['last_order'].max() - df['first_order']
    df['R_recency_days'] = df['last_order'].max() - df['last_order']
    df['F_total_orders'] = df[dow_columns].sum(axis=1)
    df['M_total_spend'] = df[cuisine_columns].sum(axis=1)
    df['avg_days_btw_orders'] = df['antiguity'] / df['F_total_orders'].replace(0, 1)
    df['avg_spend_p_order'] = df['M_total_spend'] / df['F_total_orders'].replace(0, 1)
    df['avg_products_p_order'] = df['product_count'] / df['F_total_orders'].replace(0, 1)
    df['cuisine_diversity'] = df[cuisine_columns].gt(0).sum(axis=1)
    df['chain_preference_ratio'] = df['is_chain'] / df['F_total_orders'].replace(0, 1)
    df[[f'{col}_spending_dist' for col in cuisine_columns]] = (
        df[cuisine_columns].div(df['M_total_spend'], axis=0).fillna(0).round(2)
    )
    df['promo_used'] = df['last_promo'].apply(lambda x: 1 if x != 'No Promo' else 0)
    df['weekday_avg_orders'] = df[['Mon_Orders', 'Tue_Orders', 'Wed_Orders', 'Thu_Orders', 'Fri_Orders']].mean(axis=1)
    df['weekend_avg_orders'] = df[['Sun_Orders', 'Sat_Orders']].mean(axis=1)
    df['breakfast_orders'] = df[['HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', 'HR_10']].sum(axis=1)
    df['lunch_orders'] = df[['HR_11', 'HR_12', 'HR_13', 'HR_14']].sum(axis=1)
    df['afternoon_snack_orders'] = df[['HR_15', 'HR_16', 'HR_17']].sum(axis=1)
    df['dinner_orders'] = df[['HR_18', 'HR_19', 'HR_20', 'HR_21']].sum(axis=1)
    df['late_night_orders'] = df[['HR_22', 'HR_23', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4']].sum(axis=1)
    df['day_of_week_diversity'] = df[dow_columns].gt(0).sum(axis=1)
    df['time_of_day_diversity'] = df[['breakfast_orders', 'lunch_orders', 'afternoon_snack_orders', 'dinner_orders', 'late_night_orders']].gt(0).sum(axis=1)
    df['vendor_loyalty'] = df['vendor_count'] / df['F_total_orders'].replace(0, 1)

    # Step 9: Final feature selection
    def apply_manual_filtering_final(df):
        outliers_manual = (
            (df['F_total_orders'] <= 50) &
            (df['avg_spend_p_order'] <= 100) &
            (df['breakfast_orders'] <= 25) &
            (df['lunch_orders'] <= 25) &
            (df['late_night_orders'] <= 20) &
            (df['product_count'] <= 70) &
            (df['weekend_avg_orders'] <= 10)
        )
        outliers = df[~outliers_manual]
        return df[outliers_manual], outliers

    final_df, outliers_final = apply_manual_filtering_final(df)
    outliers = pd.concat([outliers, outliers_final])

    final_df['Asian_Cuisine'] = final_df[Asian_Cuisine].sum(axis=1)
    final_df['Chinese_Cuisine'] = final_df[Chinese_Cuisine].sum(axis=1)
    final_df['Western_Cuisine'] = final_df[Western_Cuisine].sum(axis=1)
    final_df['Other_Cuisine'] = final_df[Other_Cuisine].sum(axis=1)

    final_df['Weekdays'] = final_df[['Mon_Orders', 'Tue_Orders', 'Wed_Orders', 'Thu_Orders', 'Fri_Orders']].mean(axis=1)

    scaled_df = encoder_and_scaler("standard", final_df, all_perspectives)
    scaled_df = reduce_memory_usage(scaled_df)
    final_df = reduce_memory_usage(final_df)

    return scaled_df, final_df, outliers


#---- MAIN INPUT PIP ----

# ---- Page Title ----
st.title("Cluster Prediction")

# ---- Load Data ----
@st.cache_data
def load_data():
    """
    Load and preprocess datasets.
    """
    df = pd.read_csv("DM2425_ABCDEats_DATASET.csv", sep=',')
    df_w_clusters = pd.read_csv("DM2425_ABCDEats_DATASET_w_Clusters.csv", sep=',')

    # Set customer_id as the index
    df = df.drop_duplicates(subset='customer_id').set_index('customer_id')
    df_w_clusters = df_w_clusters.set_index('customer_id')
    return df, df_w_clusters

df, df_w_clusters = load_data()

# ---- Helper Functions ----
def extract_column_info(df):
    """
    Extract unique values and formats for each column in the dataset.
    """
    column_info = {}
    for col in df.columns:
        if df[col].dtype == 'object':
            column_info[col] = df[col].dropna().unique().tolist()
        elif np.issubdtype(df[col].dtype, np.number):
            column_info[col] = (df[col].min(), df[col].max())
        elif np.issubdtype(df[col].dtype, np.datetime64):
            column_info[col] = "Date format (YYYY-MM-DD)"
    return column_info

# Import the `prepare_data` function (assumed stored in memory)

# ---- Main Application ----
def main():
    # Extract column information for inputs
    column_info = extract_column_info(df)

    st.write("## Enter Customer Details")
    input_data = {}

    # Create inputs dynamically based on column types
    for col, info in column_info.items():
        if col not in df.columns:
            continue
        if isinstance(info, list):  # Categorical
            input_data[col] = st.selectbox(f"{col}", [""] + info)
        elif isinstance(info, tuple):  # Numerical
            min_val, max_val = info
            input_data[col] = st.number_input(
                f"{col}",
                min_value=float(min_val),
                max_value=float(max_val),
                value=float(min_val) if min_val is not None else 0.0,
                step=1.0
            )
        elif info == "Date format (YYYY-MM-DD)":  # Date
            input_data[col] = st.date_input(f"{col}")
        else:  # Fallback
            input_data[col] = st.text_input(f"{col}")

    # ---- Prediction Logic ----
    if st.button("Predict Outcome"):
        try:
            # Convert user input to DataFrame
            input_df = pd.DataFrame([input_data])

            # Preprocess the input data
            scaled_input, final_input, outlier = prepare_data(input_df)

            if outlier is None:
                # Find the nearest cluster in the dataset
                X_clusters = df_w_clusters[all_perspectives]
                y_clusters = df_w_clusters['final_labels']

                # Measure distance and predict
                cluster_distances = ((X_clusters - final_input.values) ** 2).sum(axis=1)
                closest_cluster = y_clusters.loc[cluster_distances.idxmin()]
                st.success(f"The record belongs to Cluster: {closest_cluster}")
            else:
                # Handle outlier by reclassification
                st.warning("Your record is an outlier. Reclassification using Decision Tree in progress...")

                # Prepare data for decision tree
                all_features = df_w_clusters[all_perspectives]
                all_labels = df_w_clusters['final_labels']

                X_train, X_test, y_train, y_test = train_test_split(
                    all_features, all_labels, test_size=0.3, random_state=5
                )

                # Train decision tree
                dt = DecisionTreeClassifier(random_state=5, max_depth=3)
                dt.fit(X_train, y_train)

                # Predict outlier cluster
                outlier['final_labels'] = dt.predict(outlier)
                st.success(f"The record was reclassified to Cluster: {outlier['final_labels'][0]}")

                # Show model performance
                accuracy = dt.score(X_test, y_test) * 100
                st.info(f"The decision tree estimates {accuracy:.2f}% accuracy on customer classification.")

        except Exception as e:
            st.error(f"Error during prediction: {e}")

# ---- Run Application ----
if __name__ == "__main__":
    main()
